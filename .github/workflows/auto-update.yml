name: Auto-update political data

on:
  schedule:
    # Run every 15 minutes starting at 12:15am ET (UTC-5 / EST).
    # Three entries cover the full day while skipping the 12:00am ET midnight slot.
    - cron: '15,30,45 5 * * *'       # 12:15 am – 12:45 am ET
    - cron: '0,15,30,45 6-23 * * *'  #  1:00 am –  6:59 pm ET
    - cron: '0,15,30,45 0-4 * * *'   #  7:00 pm – 11:45 pm ET
  workflow_dispatch:

# One run at a time; cancel any in-progress run when a new trigger fires.
# For a data-fetch cron this is correct: the newest run always has the
# freshest data, and a stuck/slow run should never block the next slot.
concurrency:
  group: auto-update
  cancel-in-progress: true

jobs:
  fetch-and-push:
    runs-on: ubuntu-latest
    timeout-minutes: 10   # cron fires every 15 min — must finish well inside that window
    permissions:
      contents: write

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          token: ${{ github.token }}   # ensures push credentials are configured

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache npm downloads
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: npm-claude-cli-${{ runner.os }}

      - name: Install Claude CLI (Claude fallback path)
        continue-on-error: true   # primary path is direct HTTP; CLI is best-effort
        run: npm install -g @anthropic-ai/claude-code

      - name: Configure VerbAI MCP (Claude fallback path)
        continue-on-error: true   # primary path is direct HTTP; CLI config is best-effort
        env:
          VERBAI_TOKEN: ${{ secrets.VERBAI_TOKEN }}
        run: |
          claude mcp add --transport http verb-ai-mcp \
            https://zknnynm-exc60781.snowflakecomputing.com/api/v2/databases/KAFKA_DATA/schemas/DOORDASH_EVENTS/mcp-servers/VERB_AI_MCP_SERVER \
            --header "Authorization: Bearer $VERBAI_TOKEN"

      - name: Run fetch_data.py
        env:
          VERBAI_TOKEN: ${{ secrets.VERBAI_TOKEN }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: python3 fetch_data.py

      - name: Commit and push updated data
        run: |
          git config user.name  "Claude Auto-Update"
          git config user.email "noreply@anthropic.com"
          git add political_data.json history.json mcp_status.json live_feed.json sources_cache.json polling_data.json
          # Only commit if something changed
          if git diff --cached --quiet; then
            echo "No changes — skipping commit."
          else
            git commit -m "chore: auto-update data [$(date -u '+%Y-%m-%dT%H:%M:%SZ')]"
            # Rebase on top of any commits pushed to the remote while this run
            # was in flight (e.g. developer hotfixes).  -X ours keeps our fresh
            # data files on conflict so the cron result is never silently dropped.
            git fetch origin ${{ github.ref_name }}
            git rebase origin/${{ github.ref_name }} -X ours
            git push origin HEAD:${{ github.ref_name }}
          fi
